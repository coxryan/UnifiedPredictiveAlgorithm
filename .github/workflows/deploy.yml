name: deploy

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      purge_caches:
        description: 'Delete cached API data before run (true/false)'
        required: false
        default: 'false'
      enable_cache:
        description: 'Use cached API data (true/false)'
        required: false
        default: 'false'
      enable_backtest:
        description: 'Run collector backtest pass (true/false)'
        required: false
        default: 'false'
      backtest_year:
        description: 'Backtest season to run when enabled'
        required: false
        default: '2024'

permissions:
  contents: write
  pages: write
  id-token: write

env:
  MARKET_SOURCE: fanduel
  ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
  ODDS_CACHE_DIR: .cache_odds/2025
  ODDS_CACHE_TTL_DAYS: 2
  # Ensure Actions cache paths match code defaults
  CACHE_DIR: .cache_cfbd/2025
  UPA_LOG_LEVEL: DEBUG
  UPA_AUTO_GIT_ADD: 0
  CACHE_VERSION: v6
  CACHE_ENABLED: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.enable_cache == 'true' && 'true' || 'false' }}
  BACKTEST_ENABLED: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.enable_backtest == 'true' && 'true' || 'false' }}
  BACKTEST_YEAR: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.backtest_year || '2024' }}

# Serialize with other Pages publishers to avoid races
concurrency:
  group: pages
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Verify cache path configuration (fail only on mismatch)
        run: |
          set -euo pipefail
          CFBD_ENV_DIR="${CACHE_DIR:-.cache_cfbd/2025}"
          ODDS_ENV_DIR="${ODDS_CACHE_DIR:-.cache_odds/2025}"
          CFBD_EXPECT=".cache_cfbd/2025"
          ODDS_EXPECT=".cache_odds/2025"
          echo "CFBD cache (env): ${CFBD_ENV_DIR}  | expected workflow path: ${CFBD_EXPECT}"
          echo "ODDS cache (env): ${ODDS_ENV_DIR}  | expected workflow path: ${ODDS_EXPECT}"
          if [ "${CFBD_ENV_DIR}" != "${CFBD_EXPECT}" ] || [ "${ODDS_ENV_DIR}" != "${ODDS_EXPECT}" ]; then
            echo "::error::Cache path mismatch detected. Update either the workflow cache paths or the env values (CACHE_DIR/ODDS_CACHE_DIR) to match."
            echo "Details: CFBD_ENV_DIR='${CFBD_ENV_DIR}' vs EXPECT='${CFBD_EXPECT}', ODDS_ENV_DIR='${ODDS_ENV_DIR}' vs EXPECT='${ODDS_EXPECT}'"
            exit 1
          fi
          echo "Cache paths are consistent."

      - name: Purge caches (optional)
        if: ${{ env.CACHE_ENABLED == 'true' && github.event_name == 'workflow_dispatch' && github.event.inputs.purge_caches == 'true' }}
        run: |
          set -euo pipefail
          rm -rf .cache_cfbd/2025 .cache_odds/2025
          echo "Caches purged per workflow_dispatch input."

      - name: Restore API caches (CFBD & Odds)
        if: ${{ env.CACHE_ENABLED == 'true' }}
        uses: actions/cache@v4
        with:
          path: |
            .cache_cfbd/2025
            .cache_odds/2025
          key: api-caches-${{ runner.os }}-${{ env.MARKET_SOURCE }}-season-2025-${{ env.CACHE_VERSION }}-${{ hashFiles('agents/collect_cfbd_all.py') }}
          restore-keys: |
            api-caches-${{ runner.os }}-${{ env.MARKET_SOURCE }}-season-2025-${{ env.CACHE_VERSION }}-
            api-caches-${{ runner.os }}-${{ env.MARKET_SOURCE }}-season-2025-
            api-caches-${{ runner.os }}-${{ env.MARKET_SOURCE }}-
            api-caches-${{ runner.os }}-

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r agents/requirements.txt

      - name: Check CFBD token (warning only)
        run: |
          if [ -z "${{ secrets.CFBD_BEARER_TOKEN }}" ]; then
            echo "::warning::CFBD_BEARER_TOKEN secret is empty. Without it, schedule may be tiny and FanDuel mapping will be skipped."
          else
            echo "CFBD_BEARER_TOKEN is present."
          fi

      - name: Run collector (live + optional backtest)
        env:
          CFBD_BEARER_TOKEN: ${{ secrets.CFBD_BEARER_TOKEN }}
          BEARER_TOKEN: ${{ secrets.CFBD_BEARER_TOKEN }}
          MARKET_SOURCE: ${{ env.MARKET_SOURCE }}
          ODDS_API_KEY: ${{ env.ODDS_API_KEY }}
          ODDS_CACHE_DIR: ${{ env.ODDS_CACHE_DIR }}
          ODDS_CACHE_TTL_DAYS: ${{ env.ODDS_CACHE_TTL_DAYS }}
          CACHE_DIR: ${{ env.CACHE_DIR }}
          DEBUG_MARKET: 1
          REQUIRE_SCHED_MIN_ROWS: 0
          MARKET_MIN_ROWS: 1
        run: |
          # Run main collector; market_debug.json will be emitted by the script when available
          BACKTEST_ARGS=""
          if [ "${BACKTEST_ENABLED}" = "true" ]; then
            echo "Backtest enabled for season ${BACKTEST_YEAR}"
            BACKTEST_ARGS="--backtest ${BACKTEST_YEAR}"
          else
            echo "Backtest disabled for this run."
          fi
          echo "MARKET_SOURCE (requested): ${MARKET_SOURCE}"
          python -m agents.collect_cfbd_all --market-source "${MARKET_SOURCE}" --year 2025 ${BACKTEST_ARGS}
          echo "Collector finished; status.json:"
          test -f data/status.json && cat data/status.json || echo "status.json not found"
          echo "market_debug.json (if present):"
          test -f data/market_debug.json && cat data/market_debug.json || echo "market_debug.json not found"

      # Build every artifact needed for the site (always regenerate)
      - name: Build ALL site data (always regenerate)
        env:
          CFBD_BEARER_TOKEN: ${{ secrets.CFBD_BEARER_TOKEN }}
          MARKET_SOURCE: ${{ env.MARKET_SOURCE }}
          ODDS_API_KEY: ${{ env.ODDS_API_KEY }}
          CACHE_DIR: ${{ env.CACHE_DIR }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, json, sys
          from datetime import datetime, timedelta
          import pandas as pd
          from agents.collect import (
            CfbdClients,
            ApiCache,
            DATA_DIR,
            write_csv,
            load_schedule_for_year,
            discover_current_week,
            get_market_lines_for_current_week,
            build_team_inputs_datadriven,
            build_predictions_for_year,
            build_live_edge_report,
          )
          # Utilities
          DATA = os.environ.get("DATA_DIR","data")
          YEAR = int(os.environ.get("YEAR","2025"))
          os.makedirs(DATA, exist_ok=True)
          apis = CfbdClients(bearer_token=os.environ.get("CFBD_BEARER_TOKEN",""))
          cache = ApiCache()
          # 1) TEAM INPUTS
          ti_path = os.path.join(DATA, "upa_team_inputs_datadriven_v0.csv")
          ti = build_team_inputs_datadriven(YEAR, apis, cache)
          write_csv(ti, ti_path)
          print(f"[OK] wrote {ti_path} rows={len(ti)}")
          # 2) SCHEDULE (fresh/refetched if stale per loader heuristic)
          sched = load_schedule_for_year(YEAR, apis, cache)
          write_csv(sched, os.path.join(DATA, "cfb_schedule.csv"))
          print(f"[OK] wrote schedule rows={len(sched)}")
          # 3) MARKET (FanDuel or CFBD per env); write CSV always, JSON already handled by debug entry
          wk = discover_current_week(sched) or 1
          mkt = get_market_lines_for_current_week(YEAR, wk, sched, apis, cache)
          market_path = os.path.join(DATA, "market_debug.csv")
          combined_mkt = mkt
          if os.path.exists(market_path):
            try:
              existing = pd.read_csv(market_path)
              combined_mkt = pd.concat([existing, mkt], ignore_index=True)
              key_cols = [c for c in ["game_id", "week", "home_team", "away_team"] if c in combined_mkt.columns]
              if key_cols:
                combined_mkt = combined_mkt.drop_duplicates(subset=key_cols, keep="last")
              else:
                combined_mkt = combined_mkt.drop_duplicates()
            except Exception as e:
              print(f"[warn] unable to merge existing market_debug: {e}")
              combined_mkt = mkt
          if "market_spread_book" not in combined_mkt.columns and "spread" in combined_mkt.columns:
            combined_mkt = combined_mkt.assign(market_spread_book=combined_mkt["spread"])
          write_csv(combined_mkt, market_path)

          def _spread_count(df):
            for col in ("spread", "market_spread_book"):
              if col in df.columns:
                try:
                  return int(pd.to_numeric(df.get(col), errors="coerce").notna().sum())
                except Exception:
                  return 0
            return 0

          new_non_null = _spread_count(mkt)
          combined_non_null = _spread_count(combined_mkt)
          print(f"[OK] wrote market_debug rows={len(combined_mkt)} (combined, weeks <= {wk}; new rows={len(mkt)}, new spreads={new_non_null}, total spreads={combined_non_null})")
          # Ensure unmatched CSV exists (empty if matcher didn't emit one)
          unmatched_p = os.path.join(DATA, "market_unmatched.csv")
          if not os.path.exists(unmatched_p):
            pd.DataFrame(columns=["date","home_name","away_name","reason","h_best","h_score","a_best","a_score"]).to_csv(unmatched_p, index=False)
          # 4) LIVE SCORES (Eastern day)
          try:
            from agents.fetch_live_scores import fetch_scoreboard
            rows = fetch_scoreboard(None)
            pd.DataFrame(rows).to_csv(os.path.join(DATA, "live_scores.csv"), index=False)
            print(f"[OK] wrote live_scores rows={len(rows)}")
          except Exception as e:
            print(f"[warn] live scores fetch failed: {e}")
            ls_p = os.path.join(DATA, "live_scores.csv")
            if not os.path.exists(ls_p):
              pd.DataFrame(columns=["event_id","date","state","detail","clock","period","venue","home_team","away_team","home_school","away_school","home_points","away_points"]).to_csv(ls_p, index=False)
          # 5) PREDICTIONS
          preds_p = os.path.join(DATA, "upa_predictions.csv")
          preds = build_predictions_for_year(
            YEAR,
            sched,
            apis=apis,
            cache=cache,
            markets_df=combined_mkt,
            team_inputs_df=ti,
          )
          write_csv(preds, preds_p)
          preds_rows = len(preds)
          print(f"[OK] wrote predictions rows={preds_rows}")
          # 6) LIVE EDGE
          edge_p = os.path.join(DATA, "live_edge_report.csv")
          edge = build_live_edge_report(YEAR, preds_df=preds)
          write_csv(edge, edge_p)
          print(f"[OK] wrote live_edge_report rows={len(edge)}")
          # 7) STATUS + BACKFILL SUMMARY
          # Update status.json with counts
          status_p = os.path.join(DATA, "status.json")
          now = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
          try:
            teams = pd.read_csv(ti_path) if os.path.exists(ti_path) else pd.DataFrame()
            games = pd.read_csv(os.path.join(DATA, "cfb_schedule.csv"))
            preds = pd.read_csv(preds_p)
            status = {}
            if os.path.exists(status_p):
              try:
                status = json.load(open(status_p))
              except Exception:
                status = {}
            status.update({
              "generated_at_utc": now,
              "year": YEAR,
              "teams": int(len(teams)) if not teams.empty else 0,
              "games": int(len(games)) if not games.empty else 0,
              "pred_rows": int(len(preds)) if not preds.empty else 0,
              "next_run_eta_utc": now,
            })
            json.dump(status, open(status_p,"w"), indent=2)
            print(f"[OK] wrote status.json")
          except Exception as e:
            print(f"[warn] status update failed: {e}")
          # Backfill summary for Status page link
          try:
            sched = pd.read_csv(os.path.join(DATA, "cfb_schedule.csv"))
            preds = pd.read_csv(preds_p)
            out = {
              "file": "upa_predictions.csv",
              "predictions_rows": int(len(preds)),
              "predictions_rows_with_market": int(pd.to_numeric(preds.get("market_spread_book"), errors="coerce").notna().sum()) if "market_spread_book" in preds.columns else 0,
              "schedule_rows": int(len(sched)),
              "schedule_rows_with_market": int(pd.to_numeric(sched.get("market_spread_book"), errors="coerce").notna().sum()) if "market_spread_book" in sched.columns else 0,
            }
            open(os.path.join(DATA, "market_predictions_backfill.json"),"w").write(json.dumps(out, indent=2))
            print("[OK] wrote market_predictions_backfill.json")
          except Exception as e:
            print(f"[warn] unable to write backfill summary: {e}")
          PY

      # Hard validation: fail build if core artifacts are empty or stale
      - name: Validate data completeness (hard fail)
        env:
          YEAR: 2025
        run: |
          set -euo pipefail
          python - <<'PY'
          import sys, os
          import pandas as pd
          from datetime import datetime, timedelta
          DATA = "data"
          fail = []

          # 1) Schedule must exist, be sizeable, and extend to >= today-1 (US/Pacific)
          try:
            sched = pd.read_csv(os.path.join(DATA, "cfb_schedule.csv"))
            if len(sched) < 200:
              fail.append(f"schedule too small: {len(sched)} rows")
            dts = pd.to_datetime(sched.get("date"), errors="coerce")
            if dts.isna().all():
              fail.append("schedule has no valid dates")
            else:
              if hasattr(dts, "dt") and getattr(dts.dt, "tz", None) is not None:
                try:
                  dts = dts.dt.tz_convert("America/Los_Angeles")
                except Exception:
                  try:
                    dts = dts.dt.tz_localize("America/Los_Angeles")
                  except Exception:
                    pass
                try:
                  dts = dts.dt.tz_localize(None)
                except Exception:
                  pass
              max_d = dts.dropna().max()
              try:
                today_pt = pd.Timestamp.now(tz="America/Los_Angeles").date()
              except Exception:
                today_pt = pd.Timestamp.utcnow().date()
              max_date = None
              if isinstance(max_d, pd.Timestamp):
                max_date = max_d.date()
              elif pd.notna(max_d):
                try:
                  max_date = pd.Timestamp(max_d).date()
                except Exception:
                  max_date = None
              if max_date and max_date < (today_pt - timedelta(days=1)):
                fail.append(f"schedule stale: max date {max_date} < (today-1)")
          except Exception as e:
            fail.append(f"schedule load error: {e}")

          # 2) Predictions must exist and have rows
          try:
            preds = pd.read_csv(os.path.join(DATA, "upa_predictions.csv"))
            if len(preds) == 0:
              print("::warning::predictions empty (0 rows)")
          except Exception as e:
            fail.append(f"predictions load error: {e}")

          # 3) Market debug should have at least some rows
          try:
            mdbg = pd.read_csv(os.path.join(DATA, "market_debug.csv"))
            if len(mdbg) == 0:
              fail.append("market_debug empty (0 rows)")
          except Exception as e:
            fail.append(f"market_debug load error: {e}")

          # 4) Live scores file must be readable (row count can be 0 on non-slate days)
          try:
            _ = pd.read_csv(os.path.join(DATA, "live_scores.csv"))
          except Exception as e:
            fail.append(f"live_scores load error: {e}")

          if fail:
            print("::error::Data validation failed:")
            for f in fail: print(" -", f)
            sys.exit(1)
          print("[OK] data validation passed.")
          PY

      - name: Run Python unit tests
        run: python -m pytest --maxfail=1 --disable-warnings

      - name: Install deps
        run: |
          if [ -f package-lock.json ]; then
            npm ci
          else
            npm i
          fi

      - name: Build UI
        run: npm run build

      - name: Copy data into dist
        run: |
          mkdir -p dist/data
          if [ -d data ]; then
            cp -R data/* dist/data/ || true
          fi
          if [ -d public ]; then
            cp -R public/* dist/ || true
          fi

      - name: Stage generated data artifacts
        run: |
          set -euo pipefail
          if [ -d data ]; then
            git add data/
          fi
          if [ -d dist ]; then
            git add dist/
          fi
          git status --short

      - name: Commit and push generated artifacts
        run: |
          set -euo pipefail
          if git diff --cached --quiet; then
            echo "No generated changes to commit."
          else
            git config user.name "GitHub Actions"
            git config user.email "actions@github.com"
            git commit -m "chore: update generated data [skip ci]"
            git push
          fi

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: dist

      - name: Upload portable site bundle (for live workflow)
        uses: actions/upload-artifact@v4
        with:
          name: site-bundle
          path: dist
          retention-days: 3

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
