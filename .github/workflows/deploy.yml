name: deploy

on:
  push:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

env:
  MARKET_SOURCE: fanduel
  ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
  ODDS_CACHE_DIR: .cache_odds/2025
  ODDS_CACHE_TTL_DAYS: 2
  # Ensure Actions cache paths match code defaults
  CACHE_DIR: .cache_cfbd/2025

# Serialize with other Pages publishers to avoid races
concurrency:
  group: pages
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Verify cache path configuration (fail only on mismatch)
        run: |
          set -euo pipefail
          CFBD_ENV_DIR="${CACHE_DIR:-.cache_cfbd/2025}"
          ODDS_ENV_DIR="${ODDS_CACHE_DIR:-.cache_odds/2025}"
          CFBD_EXPECT=".cache_cfbd/2025"
          ODDS_EXPECT=".cache_odds/2025"
          echo "CFBD cache (env): ${CFBD_ENV_DIR}  | expected workflow path: ${CFBD_EXPECT}"
          echo "ODDS cache (env): ${ODDS_ENV_DIR}  | expected workflow path: ${ODDS_EXPECT}"
          if [ "${CFBD_ENV_DIR}" != "${CFBD_EXPECT}" ] || [ "${ODDS_ENV_DIR}" != "${ODDS_EXPECT}" ]; then
            echo "::error::Cache path mismatch detected. Update either the workflow cache paths or the env values (CACHE_DIR/ODDS_CACHE_DIR) to match."
            echo "Details: CFBD_ENV_DIR='${CFBD_ENV_DIR}' vs EXPECT='${CFBD_EXPECT}', ODDS_ENV_DIR='${ODDS_ENV_DIR}' vs EXPECT='${ODDS_EXPECT}'"
            exit 1
          fi
          echo "Cache paths are consistent."

      - name: Restore API caches (CFBD & Odds)
        uses: actions/cache@v4
        with:
          path: |
            .cache_cfbd/2025
            .cache_odds/2025
          key: api-caches-${{ runner.os }}-${{ env.MARKET_SOURCE }}-season-2025-v3-${{ hashFiles('agents/collect_cfbd_all.py') }}
          restore-keys: |
            api-caches-${{ runner.os }}-${{ env.MARKET_SOURCE }}-season-2025-
            api-caches-${{ runner.os }}-${{ env.MARKET_SOURCE }}-
            api-caches-${{ runner.os }}-

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r agents/requirements.txt

      - name: Check CFBD token (warning only)
        run: |
          if [ -z "${{ secrets.CFBD_BEARER_TOKEN }}" ]; then
            echo "::warning::CFBD_BEARER_TOKEN secret is empty. Without it, schedule may be tiny and FanDuel mapping will be skipped."
          else
            echo "CFBD_BEARER_TOKEN is present."
          fi

      - name: Run collector (live + backtest)
        env:
          BEARER_TOKEN: ${{ secrets.CFBD_BEARER_TOKEN }}
          MARKET_SOURCE: ${{ env.MARKET_SOURCE }}
          ODDS_API_KEY: ${{ env.ODDS_API_KEY }}
          ODDS_CACHE_DIR: ${{ env.ODDS_CACHE_DIR }}
          ODDS_CACHE_TTL_DAYS: ${{ env.ODDS_CACHE_TTL_DAYS }}
          CACHE_DIR: ${{ env.CACHE_DIR }}
          DEBUG_MARKET: 1
          REQUIRE_SCHED_MIN_ROWS: 0
          MARKET_MIN_ROWS: 1
        run: |
          # Run main collector; market_debug.json will be emitted by the script when available
          echo "MARKET_SOURCE (requested): ${MARKET_SOURCE}"
          python agents/collect_cfbd_all.py --market-source "${MARKET_SOURCE}" --year 2025 --backtest 2024
          echo "Collector finished; status.json:"
          test -f data/status.json && cat data/status.json || echo "status.json not found"
          echo "market_debug.json (if present):"
          test -f data/market_debug.json && cat data/market_debug.json || echo "market_debug.json not found"


      # New step: Build ALL site data (always regenerate)
      - name: Build ALL site data (always regenerate)
        env:
          CFBD_BEARER_TOKEN: ${{ secrets.CFBD_BEARER_TOKEN }}
          MARKET_SOURCE: ${{ env.MARKET_SOURCE }}
          ODDS_API_KEY: ${{ env.ODDS_API_KEY }}
          CACHE_DIR: ${{ env.CACHE_DIR }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, json, sys
          from datetime import datetime, timedelta
          import pandas as pd
          from agents.collect import (
            CfbdClients, ApiCache, DATA_DIR, write_csv, _dbg,
            load_schedule_for_year, discover_current_week,
            get_market_lines_for_current_week,
          )
          # Optional builders (may not exist in all forks)
          try:
            from agents.collect.team_inputs import build_team_inputs_datadriven
          except Exception:
            build_team_inputs_datadriven = None
          try:
            # If your repo has a real predictions builder, import it here.
            # Provide a function that returns a DataFrame with prediction columns.
            from agents.build_predictions import build_predictions_for_year  # type: ignore
          except Exception:
            build_predictions_for_year = None
          try:
            # Optional live edge builder
            from agents.build_live_edge import build_live_edge_report  # type: ignore
          except Exception:
            build_live_edge_report = None
          # Utilities
          DATA = os.environ.get("DATA_DIR","data")
          YEAR = int(os.environ.get("YEAR","2025"))
          os.makedirs(DATA, exist_ok=True)
          apis = CfbdClients(bearer_token=os.environ.get("CFBD_BEARER_TOKEN",""))
          cache = ApiCache()
          # 1) TEAM INPUTS
          ti_path = os.path.join(DATA, "upa_team_inputs_datadriven_v0.csv")
          if build_team_inputs_datadriven:
            try:
              ti = build_team_inputs_datadriven(YEAR, apis, cache)
              write_csv(ti, ti_path)
              print(f"[OK] wrote {ti_path} rows={len(ti)}")
            except Exception as e:
              print(f"[warn] team inputs build failed: {e}")
              if not os.path.exists(ti_path):
                pd.DataFrame(columns=["team","conference","wrps_percent_0_100","talent_score_0_100","portal_net_0_100"]).to_csv(ti_path, index=False)
          else:
            if not os.path.exists(ti_path):
              pd.DataFrame(columns=["team","conference","wrps_percent_0_100","talent_score_0_100","portal_net_0_100"]).to_csv(ti_path, index=False)
          # 2) SCHEDULE (fresh/refetched if stale per loader heuristic)
          sched = load_schedule_for_year(YEAR, apis, cache)
          write_csv(sched, os.path.join(DATA, "cfb_schedule.csv"))
          print(f"[OK] wrote schedule rows={len(sched)}")
          # 3) MARKET (FanDuel or CFBD per env); write CSV always, JSON already handled by debug entry
          wk = discover_current_week(sched) or 1
          mkt = get_market_lines_for_current_week(YEAR, wk, sched, apis, cache)
          write_csv(mkt, os.path.join(DATA, "market_debug.csv"))
          print(f"[OK] wrote market_debug rows={len(mkt)} for weeks <= {wk}")
          # Ensure unmatched CSV exists (em...